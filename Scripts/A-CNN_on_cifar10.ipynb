{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this script I am creating an advanced model named A-CNN (Absolute-CNN) for the processed dataset. The Absolute CNN is a model that is built upon the AANN model -> https://openreview.net/pdf?id=rkhxwltab\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I am using the Cifar-10 dataset for this script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with importing the usual cells for my tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for pickling the data\n",
    "import cPickle as pickle\n",
    "\n",
    "# THE TensorFlow framework\n",
    "import tensorflow as tf\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\n",
      "LICENSE\n",
      "Localizer\n",
      "Models\n",
      "README.md\n",
      "Scripts\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls', '..'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data/cifar-10-batches-py\" # the data path\n",
    "train_meta = os.path.join(data_path, \"batches.meta\")\n",
    "base_model_path = '../Models/A-CNN/Cifar'\n",
    "\n",
    "# constant values:\n",
    "im_dim = 32 # the images of size 32 x 32\n",
    "n_channels = 3 # RGB channels\n",
    "highest_pixel_value = 255.0 # 8 bits for every channel. So, max value is 255\n",
    "no_of_epochs = 100 # No. of epochs to run\n",
    "no_of_batches = 5 # There are 5 batches in the dataset\n",
    "checkpoint_factor = 5 # save the model after every 5 steps (epochs)\n",
    "neurons_in_attention_generator = 512\n",
    "num_filters = 64 # The filters will increase in this order\n",
    "batch_size = 32 # The batch size for training\n",
    "lr = 0.01 # The learning rate for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/cifar-10-batches-py ../Models/A-CNN/Cifar\n"
     ]
    }
   ],
   "source": [
    "print data_path, base_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.meta\n",
      "data_batch_1\n",
      "data_batch_2\n",
      "data_batch_3\n",
      "data_batch_4\n",
      "data_batch_5\n",
      "readme.html\n",
      "test_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the contents inside the data folder\n",
    "exec_command(['ls', data_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Labels sequence here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane',\n",
    "                'automobile',\n",
    "                'bird',\n",
    "                'cat',\n",
    "                'deer',\n",
    "                'dog',\n",
    "                'frog',\n",
    "                'horse',\n",
    "                'ship',\n",
    "                'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also define the number of labels\n",
    "num_labels = len(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the unpickler function from the helpers repository to get the processed data into memory:\n",
    "link to code -> https://github.com/akanimax/machine-learning-helpers/blob/master/pickling_unpickling/pickling_operations.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to unpickle the given file and load the obj back into the python environment\n",
    "def unPickleIt(pickle_path): # might throw the file not found exception\n",
    "    '''\n",
    "        function to unpickle the object from the given path\n",
    "        @param\n",
    "        pickle_path => the path where the pickle file is located\n",
    "        @return => the object extracted from the saved path\n",
    "    '''\n",
    "\n",
    "    with open(pickle_path, 'rb') as dumped_pickle:\n",
    "        obj = pickle.load(dumped_pickle)\n",
    "\n",
    "    return obj # return the unpickled object\n",
    "\n",
    "# The batch generator function:\n",
    "def generateBatch(batchFile):\n",
    "    '''\n",
    "        The function to generate a batch of data suitable for performing the convNet operations on it\n",
    "        @param batchFile -> the path of the input batchfile\n",
    "        @return batch: (data, labels) -> the processed data.\n",
    "    '''\n",
    "    # unpickle the batch file:\n",
    "    data_dict = unPickleIt(batchFile)\n",
    "    \n",
    "    # extract the data and labels from this dictionary\n",
    "    unprocessed_data = data_dict['data']\n",
    "    integer_labels = np.array(data_dict['labels']) # labels in integer form\n",
    "    \n",
    "    # reshape and rotate the data\n",
    "    data = unprocessed_data.reshape((len(unprocessed_data), im_dim, im_dim, n_channels), order='F')\n",
    "    processed_data = np.array(map(lambda x: np.rot90(x, axes=(1, 0)), data))\n",
    "    \n",
    "    # normalize the images by dividing all the pixels by 255\n",
    "    processed_data = processed_data.astype(np.float32) / 255\n",
    "    \n",
    "    # return the processed data and the encoded_labels:\n",
    "    return (processed_data, integer_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''\n",
    "        function to load all the present data in the disk into the RAM.\n",
    "        @param => none\n",
    "        @return => (images, labels)\n",
    "    '''\n",
    "    # extract data for all batches and then stack them together.\n",
    "    data_X = []; data_Y = [] # initialize to empty lists\n",
    "    for batch_number in range(1, no_of_batches + 1):\n",
    "        dat, labs = generateBatch(os.path.join(data_path, \"data_batch_\" + str(batch_number)))\n",
    "\n",
    "        # add the generated data into the data_lists\n",
    "        data_X.append(dat); data_Y.append(labs)\n",
    "\n",
    "    # finally, stack all the data together\n",
    "    data_X = np.concatenate(data_X); data_Y = np.concatenate(data_Y)\n",
    "\n",
    "    return data_X, data_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_images_shape: (50000, 32, 32, 3)\n",
      "Training_labels_shape: (50000,)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of train_X and train_Y\n",
    "print \"Training_images_shape: \" + str(train_X.shape)\n",
    "print \"Training_labels_shape: \" + str(train_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a randomized cell to load an image and display it's label.\n",
    "## Run the following cell multiple times to make sure the data is still sane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEVCAYAAAAvoDOaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRQPfsdJFA9+x0mU/wdz+JbOyQFihQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d406a9750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(train_X.shape[0])\n",
    "random_image = train_X[random_index]\n",
    "label_for_random_image = label_names[train_Y[random_index]]\n",
    "\n",
    "# use plt to display the image\n",
    "plt.figure().suptitle(\"Image label:\" + label_for_random_image)\n",
    "plt.imshow(random_image); # suppress the terminal output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build the Tensorflow computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this point to restart the cell running if something goes wrong while creating the graph below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset point for the graph:\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input placeholders and the one hot label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the placeholders for the input images and their corresponding labels:\n",
    "with tf.variable_scope(\"Input_placeholders\"):\n",
    "    tf_input = tf.placeholder(tf.float32, shape=(None, im_dim, im_dim, n_channels), name=\"Raw_input_images\")\n",
    "    tf_input_summary = tf.summary.image(\"Input_Images\", tf_input, max_outputs=10)\n",
    "    \n",
    "    tf_labels = tf.placeholder(tf.int32, shape=(None,), name=\"Ideal_image_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Input_placeholders/Raw_input_images:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'Input_placeholders/Ideal_image_labels:0' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_input, tf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the attention generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Attention_mask_generator\"):\n",
    "    \n",
    "    # apply a few conv layers with ReLU (*Note the usage of ReLU activation function*)\n",
    "    at_lay_1 = tf.layers.conv2d(tf_input, num_filters, kernel_size=(3, 3), padding=\"same\", name=\"attention_layer_1\",\n",
    "                               activation=tf.nn.relu)\n",
    "    \n",
    "    at_lay_2 = tf.layers.conv2d(at_lay_1, num_filters, kernel_size=(5, 5), padding=\"same\", name=\"attention_layer_2\",\n",
    "                               activation=tf.nn.relu)\n",
    "    \n",
    "    at_lay_3 = tf.layers.conv2d(at_lay_2, n_channels, kernel_size=(7, 7), padding=\"same\", name=\"attention_layer_3\",\n",
    "                               activation=tf.nn.relu)\n",
    "    \n",
    "    # The attention mask:\n",
    "    attention_mask = tf.nn.tanh(at_lay_3, name=\"TanH_activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes of all the intermediate tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Attention_mask_generator/attention_layer_1/Relu:0' shape=(?, 32, 32, 64) dtype=float32>,\n",
       " <tf.Tensor 'Attention_mask_generator/attention_layer_2/Relu:0' shape=(?, 32, 32, 64) dtype=float32>,\n",
       " <tf.Tensor 'Attention_mask_generator/attention_layer_3/Relu:0' shape=(?, 32, 32, 3) dtype=float32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_lay_1, at_lay_2, at_lay_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Attention_mask_generator/TanH_activation:0' shape=(?, 32, 32, 3) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Computations for the Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the attention mask to create an attention on the input image\n",
    "with tf.variable_scope(\"Attention_masker\"):\n",
    "    # This is just elementwise multiplication\n",
    "    tf_attention_masked_input = tf.multiply(tf_input, attention_mask)\n",
    "    \n",
    "    # create a summary op on this image to see what the network is focussing it's attention on right now\n",
    "    tf_attention_masked_input_summary = tf.summary.image(\"Attention_Masked_Input\", \n",
    "                                                         tf_attention_masked_input, max_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Attention_masker/Mul:0' shape=(?, 32, 32, 3) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_attention_masked_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the one hot encoder to encode the avaliable labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"One_hot_encoder\"):\n",
    "    # one hot encode the input labels\n",
    "    one_hot_encoded_tf_labels = tf.one_hot(tf_labels, depth=num_labels, axis=1, name=\"One_hot_Encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Attention_masker/Mul:0' shape=(?, 32, 32, 3) dtype=float32>,\n",
       " <tf.Tensor 'One_hot_encoder/One_hot_Encode:0' shape=(?, 10) dtype=float32>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimensions and types of the input placeholder:\n",
    "tf_attention_masked_input, one_hot_encoded_tf_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Encoder and decoder functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the convolutional encoder for generating the representations from the masked image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(images, w_reuse=None):\n",
    "    '''\n",
    "        Function to encode the input images into representation vectors\n",
    "        @param \n",
    "        images => tensor containing the input images to be trained on\n",
    "        reuse => Boolean controlling reuse of weights \n",
    "        @return => Encoded_representation_vector\n",
    "    '''\n",
    "    # set a name scope here so that we end up using the same weights and biases for the infernce mode \n",
    "    # and the decoding part:\n",
    "    with tf.name_scope(\"Encoder\"):\n",
    "        \n",
    "        # The low level feature extractors: input of (32 x 32)  running depth of (64)\n",
    "        a1 = tf.layers.conv2d(images, num_filters, kernel_size=(3, 3), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_1\", reuse=w_reuse)\n",
    "        \n",
    "        a2 = tf.layers.conv2d(a1, num_filters, kernel_size=(3, 3), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_2\", reuse=w_reuse)\n",
    "        \n",
    "        a3 = tf.layers.conv2d(a2, num_filters, kernel_size=(3, 3), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_3\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The mid level features extractors: input of (32 x 32) running  depth of (128)\n",
    "        a4 = tf.layers.conv2d(a3, (2 * num_filters), kernel_size=(5, 5),\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_4\", reuse=w_reuse)\n",
    "        \n",
    "        a5 = tf.layers.conv2d(a4, (2 * num_filters), kernel_size=(5, 5),\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_5\", reuse=w_reuse)\n",
    "        \n",
    "        a6 = tf.layers.conv2d(a5, (2 * num_filters), kernel_size=(5, 5), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_6\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The high level features extractors: input of (32 x 32) running  depth of (256)\n",
    "        a7 = tf.layers.conv2d(a6, (4 * num_filters), kernel_size=(7, 7), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_7\", reuse=w_reuse)\n",
    "        \n",
    "        a8 = tf.layers.conv2d(a7, (4 * num_filters), kernel_size=(7, 7), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_8\", reuse=w_reuse)\n",
    "        \n",
    "        a9 = tf.layers.conv2d(a8, (4 * num_filters), kernel_size=(7, 7), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_9\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The Representation vector creator layers: input of (32 x 32) running  depth of (512)\n",
    "        a10 = tf.layers.conv2d(a9, (8 * num_filters), kernel_size=(8, 8), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_10\", reuse=w_reuse)\n",
    "        \n",
    "        a11 = tf.layers.conv2d(a10, (8 * num_filters), kernel_size=(16, 16), \n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_11\", reuse=w_reuse)\n",
    "        \n",
    "        a12 = tf.layers.conv2d(a11, num_labels, kernel_size=(32, 32), \n",
    "                              padding=\"valid\", activation=tf.abs, name = \"layer_12\", reuse=w_reuse)\n",
    "        \n",
    "        # obtain the representation vector from the a12 activations by reshaping that tensor\n",
    "        representations = tf.reshape(a12, shape=(-1, num_labels))\n",
    "        \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the deconvolutional decoder for generating the original image from the representation vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(reps, w_reuse=True):\n",
    "    '''\n",
    "        Function to encode the input images into representation vectors\n",
    "        @param \n",
    "        reps => tensor containing the learned representation vectors to be deconvolved\n",
    "        reuse => Boolean controlling reuse of weights \n",
    "        @return => Encoded_representation_vector\n",
    "    '''\n",
    "    with tf.name_scope(\"Decoder\"):\n",
    "        \n",
    "        # reshape representation_vectors into an activation map\n",
    "        deconv_input = tf.reshape(reps, shape=(-1, 1, 1, num_labels))\n",
    "        \n",
    "        # The superficial feature drawers: input of (1 x 1)\n",
    "        a1 = tf.layers.conv2d_transpose(deconv_input, (8 * num_filters), kernel_size=(32, 32), use_bias=False,\n",
    "                              padding=\"valid\", activation=tf.abs, name = \"layer_12\", reuse=w_reuse)\n",
    "        \n",
    "        a2 = tf.layers.conv2d_transpose(a1, (8 * num_filters), kernel_size=(16, 16), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_11\", reuse=w_reuse)\n",
    "        \n",
    "        a3 = tf.layers.conv2d_transpose(a2, (4 * num_filters), kernel_size=(8, 8), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_10\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The high level feature drawers: input of (32 x 32)\n",
    "        a4 = tf.layers.conv2d_transpose(a3, (4 * num_filters), kernel_size=(7, 7), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_8\", reuse=w_reuse)\n",
    "        \n",
    "        a5 = tf.layers.conv2d_transpose(a4, (4 * num_filters), kernel_size=(7, 7), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_9\", reuse=w_reuse)\n",
    "        \n",
    "        a6 = tf.layers.conv2d_transpose(a5, (2 * num_filters), kernel_size=(7, 7), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_7\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The mid level feture drawers: converted input of (32 x 32)\n",
    "        a7 = tf.layers.conv2d_transpose(a6, (2 * num_filters), kernel_size=(5, 5), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_6\", reuse=w_reuse)\n",
    "        \n",
    "        a8 = tf.layers.conv2d_transpose(a7, (2 * num_filters), kernel_size=(5, 5), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_5\", reuse=w_reuse)\n",
    "        \n",
    "        a9 = tf.layers.conv2d_transpose(a8, num_filters, kernel_size=(5, 5), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_4\", reuse=w_reuse)\n",
    "        \n",
    "        \n",
    "        # The lowest (fine details) feature drawers: converted input of (32 x 32)\n",
    "        a10 = tf.layers.conv2d_transpose(a9, num_filters, kernel_size=(3, 3), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_3\", reuse=w_reuse)\n",
    "        \n",
    "        a11 = tf.layers.conv2d_transpose(a10, num_filters, kernel_size=(3, 3), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_2\", reuse=w_reuse)\n",
    "        \n",
    "        a12 = tf.layers.conv2d_transpose(a11, n_channels, kernel_size=(3, 3), use_bias=False,\n",
    "                              padding=\"same\", activation=tf.abs, name = \"layer_1\", reuse=w_reuse)\n",
    "        \n",
    "    # return the final generated image to compare it with the original\n",
    "    return a12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the functions used for computing the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def directional_cosines(inp):\n",
    "    '''\n",
    "        function to convert input tensor into it's corresponding directional cosine values\n",
    "    '''\n",
    "    with tf.variable_scope(\"Directional_cosines\"):\n",
    "        squared = tf.square(inp) # square all the numbers\n",
    "        denominator = tf.reduce_sum(squared, axis=1, keep_dims=True)\n",
    "        dc = inp / denominator\n",
    "    \n",
    "    # return the directional cosine values\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(tensor1, tensor2):\n",
    "    '''\n",
    "        function to compute the euclidean distances between two tensors\n",
    "        It is assumed that the batch dimension is the first one\n",
    "    '''\n",
    "    with tf.variable_scope(\"Euclidean_distance\"):\n",
    "        diff = tensor1 - tensor2\n",
    "        sqrd_diff = tf.square(diff)\n",
    "        sums = tf.reduce_sum(sqrd_diff, axis=1, keep_dims=True)\n",
    "        distances = tf.sqrt(sums)\n",
    "    \n",
    "    # return the computed euclidean distances:\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training and Inference Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Training_computations\"):\n",
    "    # obtain the encoded representation vectors:\n",
    "    reps = encode(tf_attention_masked_input)\n",
    "    \n",
    "    # convert the reps to directional cosines\n",
    "    dcs = directional_cosines(reps)\n",
    "    \n",
    "    # send the reps back into the deconv network to obtain the reconstructed image\n",
    "    reverse_maps = decode(reps)\n",
    "    reverse_maps_summary = tf.summary.image(\"Reverse_constructed_image\", reverse_maps, max_outputs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Training_computations/Encoder/Reshape:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Training_computations/Directional_cosines/div:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Training_computations/Decoder/layer_1/Abs:0' shape=(?, 32, 32, 3) dtype=float32>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimensions of all three\n",
    "reps, dcs, reverse_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Forward, Backward and the total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Losses\"):\n",
    "    with tf.name_scope(\"forward_loss\"):\n",
    "        fwd_loss = tf.reduce_mean(euclidean_distance(dcs, one_hot_encoded_tf_labels))\n",
    "        fwd_loss_summary = tf.summary.scalar(\"Forward_loss\", fwd_loss)\n",
    "        \n",
    "    with tf.name_scope(\"reverse_loss\"):\n",
    "        # flatten the reverse map and the attention masked input image\n",
    "        flat_reverse_maps = tf.reshape(reverse_maps, shape=(-1, (im_dim * im_dim * n_channels)))\n",
    "        flat_tf_attention_masked_input = tf.reshape(tf_attention_masked_input, \n",
    "                                                    shape=(-1, (im_dim * im_dim * n_channels)))\n",
    "        \n",
    "        bwd_loss = tf.reduce_mean(euclidean_distance(flat_reverse_maps, flat_tf_attention_masked_input))\n",
    "        bwd_loss_summary = tf.summary.scalar(\"Reverse_loss\", bwd_loss)\n",
    "        \n",
    "    with tf.name_scope(\"combined_loss\"):\n",
    "        total_loss = fwd_loss + bwd_loss\n",
    "        total_loss_summary = tf.summary.scalar(\"Total_loss\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Losses/forward_loss/Mean:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'Losses/reverse_loss/Mean:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'Losses/combined_loss/add:0' shape=() dtype=float32>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_loss, bwd_loss, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Inference computations for image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference computations need to be defined only for the reverse image generation module.\n",
    "with tf.name_scope(\"Inference_computations\"):\n",
    "    # define the placeholder to supply for generating the images:\n",
    "    manual_representations = tf.placeholder(tf.float32, shape=(None, num_labels), name=\"Image_generator_input\")\n",
    "    \n",
    "    # obtain the generated image using the decode function\n",
    "    generated_image = decode(manual_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Inference_computations/Image_generator_input:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Tensor 'Inference_computations/Decoder/layer_1/Abs:0' shape=(?, 32, 32, 3) dtype=float32>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the manual_representations and the generated_image dimensions\n",
    "manual_representations, generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimizer for optimizing the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Trainer\"):\n",
    "    # define the optimizer to train on:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    \n",
    "    # define the training_step:\n",
    "    train_step = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the usual errands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"):\n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a fake session and check if the graph is properly wired using the tensorboard visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Models/A-CNN/Cifar/Model1'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"Model1\"\n",
    "log_dir = os.path.join(base_model_path, model_name)\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the folder Model1 is empty before executing the following cell, otherwise, it will create duplicate event files and will cause tensorboard to raise warnings.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "## I also print out all the trainable variables in the graph to be sure that during inference also, we use the trained variables and not some newly created ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention_mask_generator/attention_layer_1/kernel:0\n",
      "Attention_mask_generator/attention_layer_1/bias:0\n",
      "Attention_mask_generator/attention_layer_2/kernel:0\n",
      "Attention_mask_generator/attention_layer_2/bias:0\n",
      "Attention_mask_generator/attention_layer_3/kernel:0\n",
      "Attention_mask_generator/attention_layer_3/bias:0\n",
      "layer_1/kernel:0\n",
      "layer_1/bias:0\n",
      "layer_2/kernel:0\n",
      "layer_2/bias:0\n",
      "layer_3/kernel:0\n",
      "layer_3/bias:0\n",
      "layer_4/kernel:0\n",
      "layer_4/bias:0\n",
      "layer_5/kernel:0\n",
      "layer_5/bias:0\n",
      "layer_6/kernel:0\n",
      "layer_6/bias:0\n",
      "layer_7/kernel:0\n",
      "layer_7/bias:0\n",
      "layer_8/kernel:0\n",
      "layer_8/bias:0\n",
      "layer_9/kernel:0\n",
      "layer_9/bias:0\n",
      "layer_10/kernel:0\n",
      "layer_10/bias:0\n",
      "layer_11/kernel:0\n",
      "layer_11/bias:0\n",
      "layer_12/kernel:0\n",
      "layer_12/bias:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensorboard_writer = tf.summary.FileWriter(logdir=log_dir, graph=sess.graph, filename_suffix=\".bot\")\n",
    "    \n",
    "    # initialize the session to generate the visualization file\n",
    "    sess.run(init)\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "    \n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can now write the session Code for training this graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop for this hybrid model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Models/A-CNN/Cifar/Model1/Model1\n"
     ]
    }
   ],
   "source": [
    "model_path = log_dir\n",
    "print os.path.join(model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "num_train_examples = train_X.shape[0]\n",
    "print num_train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "=================================================================================================\n",
      "=================================================================================================\n",
      "range:(0, 32) loss= 70.1889343262\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    WARNING WARNING WARNING!!! This is the main training cell.\n",
    "    This cell will take a really really long time on low-end machines. It will however not crash your pc, since \n",
    "    I have bootstrapped the training in such a way that it loads a small chunk of data at a time to train.\n",
    "'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the weights from the model1\n",
    "        # instead of global variable initializer, restore the graph:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    else:\n",
    "        # initialize all the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    g_step = 0\n",
    "    for ep in range(no_of_epochs):  # epochs loop\n",
    "        \n",
    "        print \"epoch: \" + str(ep + 1)\n",
    "        print \"=================================================================================================\"\n",
    "        print \"=================================================================================================\"\n",
    "        \n",
    "            \n",
    "        for index in range(int(np.ceil(float(num_train_examples) / batch_size))):\n",
    "            start = index * batch_size\n",
    "            end = start + batch_size\n",
    "            minX = train_X[start: end]; minY = train_Y[start: end]\n",
    "                \n",
    "            _, loss = sess.run([train_step, total_loss], feed_dict={tf_input: minX, tf_labels: minY})\n",
    "                \n",
    "            if(index % 3 == 0):\n",
    "                print('range:{} loss= {}'.format((start, start + len(minX)), loss))\n",
    "            \n",
    "            g_step += 1\n",
    "                \n",
    "        print \"\\n=========================================================================================\\n\"\n",
    "        \n",
    "        if((ep + 1) % checkpoint_factor == 0 or ep == 0):\n",
    "            \n",
    "            # calculate the summaries:\n",
    "            sums = sess.run(all_summaries, feed_dict={tf_input: minX, tf_labels: minY})\n",
    "            \n",
    "            # add the summaries to the fileWriter\n",
    "            tensorboard_writer.add_summary(sums, global_step = g_step)\n",
    "            \n",
    "            # save the model trained so far:\n",
    "            saver.save(sess, os.path.join(model_path, model_name), global_step = (ep + 1))\n",
    "        \n",
    "    print \"=================================================================================================\"\n",
    "    print \"=================================================================================================\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
